import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split 


df = pd.read_csv('Datasets/Final_df.csv')


df.columns


X = df.drop('Predicted_Production',axis=1)
y = df['Predicted_Production']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


X_train.shape


X_test.shape


X_test.head()








from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer


ohe = OneHotEncoder(drop='first')
scaler = StandardScaler()


preprocessor = ColumnTransformer(

transformers = [
    ('onehotencoder',ohe,[0,1,2]),
    ('standardization',scaler,[3,4,5])
],
remainder = 'passthrough'
)


preprocessor


fX_train = preprocessor.fit_transform(X_train)
fX_test = preprocessor.transform(X_test)





# Regression models
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.dummy import DummyRegressor

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
#from sklearn.neural_network import MLPRegressor






models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "Lasso Regression": Lasso(),
    "ElasticNet Regression": ElasticNet(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor(),
    "Gradient Boosting": GradientBoostingRegressor(),
    "Hist Gradient Boosting": HistGradientBoostingRegressor(),
    "SVR": SVR(),
    "KNN Regressor": KNeighborsRegressor(),
    "MLP Regressor": MLPRegressor(),
    "Dummy Regressor (Baseline)": DummyRegressor(strategy='mean')
}

if hasattr(fX_train, "toarray"):  # Check if the data is sparse
    fX_train = fX_train.toarray()
if hasattr(fX_test, "toarray"):
    fX_test = fX_test.toarray()

# Iterate through models and fit them
for model_name, model in models.items():
    try:
        model.fit(fX_train, y_train)
        y_pred = model.predict(fX_test)

        # Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f'{model_name} MSE: {mse:.4f} | MAE: {mae:.4f} | R2 Score: {r2:.4f}')
    except Exception as e:
        print(f"{model_name} failed: {e}")



# Ensure the data is dense


#for model_name, model in models.items():
    # Create a pipeline
   # model.fit(fX_train,y_train)
   # y_pred = model.predict(fX_test)
  #  print(f'{model_name} MSE : {mean_squared_error(y_test,y_pred)} Score {r2_score(y_test,y_pred)}')
    





dtr= DecisionTreeRegressor()
dtr.fit(fX_train,y_train)
y = dtr.predict(fX_test)




# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mean_squared_error(y_test, y))

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y)

# R² Score
r2 = r2_score(y_test, y)

# Print the evaluation metrics
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"R² Score: {r2:.2f}")








def prediction(Crop,Variety,State,Duration,Cost,Quantity):
    features = np.array([[Crop,Variety,State,Duration,Cost,Quantity]])
    transformed_features = preprocessor.transform(features)
    predicted_value = dtr.predict(transformed_features).reshape(1,-1)
    return predicted_value[0]


X_train


Crop = 'Mesta'
Variety =	"SHRESTHA (JRM-5)"
State = 'Orissa'
Duration = 134
Cost = 33198.949747
Quantity = 56.38

result = prediction(Crop,Variety,State,Duration,Cost,Quantity)



result





import pickle
#pickle.dump(dtr,open('dtr.pkl','wb'))
#pickle.dump(preprocessor,open('preprocessor.pkl','wb'))


python -c "import sklearn; print(sklearn.__version__)"


pip show scikit-learn



import sklearn
sklearn.__version__



